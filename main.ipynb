{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'std (Python 3.10.0)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import ctypes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K, Model, Input, layers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, BatchNormalization, ReLU, MaxPooling1D, GlobalAveragePooling1D, Add, Concatenate, Input, LayerNormalization, MultiHeadAttention, Flatten, Reshape, LeakyReLU, Activation, Lambda, Layer\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, Metric\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback, LearningRateScheduler\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "from tensorflow.data import Dataset\n",
    "from keras import mixed_precision\n",
    "\n",
    "# Scikit-learn for machine learning utilities\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler as sklearn_StandardScaler\n",
    "\n",
    "# PySpark for distributed data processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType, DoubleType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler as SparkVectorAssembler, MinMaxScaler as SparkMinMaxScaler, RobustScaler, StandardScaler as SparkStandardScaler, PCA as SparkPCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Keras Tuner for hyperparameter tuning\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters, Hyperband\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "# Numba for CUDA GPU acceleration\n",
    "from numba import cuda\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Logging information for dynamic error handling\n",
    "import logging\n",
    "\n",
    "# Console output coloring\n",
    "from termcolor import colored\n",
    "\n",
    "# Import sqrt function from math library\n",
    "from math import sqrt\n",
    "\n",
    "# Import count function from itertools library\n",
    "from itertools import count\n",
    "# Import mean, stddev, col, when functions from pyspark.sql.functions library\n",
    "from pyspark.sql.functions import mean, stddev, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mixed precision policy to improve performance\n",
    "# Mixed precision uses both 16-bit and 32-bit floating point types\n",
    "# This can speed up training and reduce memory usage on compatible hardware\n",
    "policy = Policy('bfloat16')\n",
    "set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session for the master node\n",
    "# 1 - Use all available local cores for the master node\n",
    "# 2 - Set the application name for the master node\n",
    "# 3 - Configure the driver and executor memory and GPU settings\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Configure Spark with GPU settings for RTX 4090 (24GB) and A5000 (24GB)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ERROR 418 - I'M A TEAPOT\") \\\n",
    "    .config(\"spark.driver.memory\", \"50g\") \\\n",
    "    .config(\"spark.executor.memory\", \"50g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.pinnedPool.size\", \"24G\") \\\n",
    "    .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.pooling.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.95\") \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"2\") \\\n",
    "    .config(\"spark.task.resource.gpu.amount\", \"0.25\") \\\n",
    "    .config(\"spark.rapids.sql.incompatibleOps.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.host.spillStorageSize\", \"64G\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.95\") \\\n",
    "    .config(\"spark.rapids.sql.batchSizeBytes\", \"512M\") \\\n",
    "    .config(\"spark.rapids.sql.reader.batchSizeRows\", \"100000\") \\\n",
    "    .config(\"spark.rapids.sql.variableRowGroupSize.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "if gpus:\n",
    "  # Replicate your computation on multiple GPUs\n",
    "  c = []\n",
    "  for gpu in gpus:\n",
    "    with tf.device(gpu.name):\n",
    "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "      c.append(tf.matmul(a, b))\n",
    "\n",
    "  with tf.device('/CPU:0'):\n",
    "    matmul_sum = tf.add_n(c)\n",
    "\n",
    "  print(matmul_sum)\n",
    "\n",
    "# Set log level to INFO\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "# Configure logging\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "logger = log4jLogger.LogManager.getLogger(__name__)\n",
    "\n",
    "def custom_logger(level, message):\n",
    "    color = 'white'\n",
    "    if level == \"INFO\":\n",
    "        color = 'cyan'\n",
    "    elif level == \"SUCCESS\":\n",
    "        color = 'green'\n",
    "    elif level == \"ERROR\":\n",
    "        color = 'red'\n",
    "    elif level == \"ACTION\":\n",
    "        color = 'blue'\n",
    "    elif level == \"PROGRESS\" or level == \"WARNING\":\n",
    "        color = 'yellow'\n",
    "    elif level == \"FINAL\":\n",
    "        color = 'magenta'\n",
    "    logger.info(colored(f\"SPARK: {level} - {message}\", color))\n",
    "\n",
    "# Set the environment variable for memory allocator\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "def initialize_cuda():\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuInit = cuda.cuInit\n",
    "    cuInit.restype = ctypes.c_int\n",
    "    cuInit.argtypes = [ctypes.c_uint]\n",
    "    cuDeviceGetCount = cuda.cuDeviceGetCount\n",
    "    cuDeviceGetCount.restype = ctypes.c_int\n",
    "    cuDeviceGetCount.argtypes = [ctypes.POINTER(ctypes.c_int)]\n",
    "    cuDeviceGet = cuda.cuDeviceGet\n",
    "    cuDeviceGet.restype = ctypes.c_int\n",
    "    cuDeviceGet.argtypes = [ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n",
    "\n",
    "    res = cuInit(0)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to initialize CUDA\")\n",
    "    device_count = ctypes.c_int()\n",
    "    res = cuDeviceGetCount(ctypes.byref(device_count))\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to get device count\")\n",
    "    devices = []\n",
    "    for i in range(device_count.value):\n",
    "        device = ctypes.c_int()\n",
    "        res = cuDeviceGet(ctypes.byref(device), i)\n",
    "        if res != 0:\n",
    "            raise RuntimeError(f\"Failed to get device {i}\")\n",
    "        devices.append(device.value)\n",
    "    return devices\n",
    "\n",
    "def allocate_gpu_memory(size):\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuda_malloc = cuda.cuMemAlloc\n",
    "    cuda_malloc.restype = ctypes.c_int\n",
    "    cuda_malloc.argtypes = [ctypes.POINTER(ctypes.c_ulonglong), ctypes.c_ulonglong]\n",
    "    ptr = ctypes.c_ulonglong()\n",
    "    res = cuda_malloc(ctypes.byref(ptr), size)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to allocate GPU memory\")\n",
    "    return ptr.value\n",
    "\n",
    "def free_gpu_memory(ptr):\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuda_free = cuda.cuMemFree\n",
    "    cuda_free.restype = ctypes.c_int\n",
    "    cuda_free.argtypes = [ctypes.c_ulonglong]\n",
    "    res = cuda_free(ptr)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to free GPU memory\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "def check_gpu_memory():\n",
    "    os.system('nvidia-smi')\n",
    "\n",
    "# Initialize CUDA\n",
    "cuda_devices = initialize_cuda()\n",
    "\n",
    "# Check available GPU memory\n",
    "check_gpu_memory()\n",
    "\n",
    "# Ensure TensorFlow uses the GPU by setting memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "# Check and print the number of GPUs available\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"Num GPUs Available: {num_gpus}\")\n",
    "\n",
    "# Perform a simple TensorFlow operation to verify GPU usage\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=tf.float32)\n",
    "    c = tf.matmul(a, b)\n",
    "print(f\"Result of matrix multiplication on GPU:\\n{c.numpy()}\")\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        memory_used = int(result.stdout.decode('utf-8').strip())\n",
    "        return memory_used\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get GPU memory usage: {e}\")\n",
    "        return None\n",
    "\n",
    "def freegpu():\n",
    "    try:\n",
    "        # Get GPU memory usage before freeing\n",
    "        memory_before = get_gpu_memory_usage()\n",
    "\n",
    "        result = subprocess.run(['sudo', 'fuser', '-v', '/dev/nvidia*'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        for line in result.stdout.decode('utf-8').split('\\n'):\n",
    "            if '/dev/nvidia' in line:\n",
    "                pid = int(line.split()[-1])\n",
    "                os.kill(pid, 9)\n",
    "\n",
    "        # Get GPU memory usage after freeing\n",
    "        memory_after = get_gpu_memory_usage()\n",
    "\n",
    "        if memory_before is not None and memory_after is not None:\n",
    "            memory_freed = memory_before - memory_after\n",
    "            print(f\"GPU memory has been freed. {memory_freed} MB of GPU memory was released.\")\n",
    "        else:\n",
    "            print(\"GPU memory has been freed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to free GPU memory: {e}\")\n",
    "\n",
    "# Set the environment variable for XLA flags\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n",
    "\n",
    "# ======================================================================================================\n",
    "# ==============================[SETUP MIRROREDSTRATEGY WITHOUT NCCL]===================================\n",
    "# ======================================================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Define the strategy with HierarchicalCopyAllReduce to avoid NCCL\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    devices=[\"/gpu:0\", \"/gpu:1\"],\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",
    ")\n",
    "\n",
    "print(f\"Number of devices under strategy: {strategy.num_replicas_in_sync}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVARIATE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data for autoencoder training\n",
    "original_data_train = pd.read_csv('/home/skander/datasets/kaggle/compet/mental_health_2024/mental_test.csv', index_col=False, header=0)\n",
    "original_data_test = pd.read_csv('/home/skander/datasets/kaggle/compet/mental_health_2024/mental_train.csv', index_col=False, header=0)\n",
    "\n",
    "original_data = pd.concat([original_data_train, original_data_test], ignore_index=True)\n",
    "\n",
    "custom_logger(\"INFO\", \"ORIGINAL DATA - BEFORE PROCESSING\")\n",
    "\n",
    "# Univariate analysis\n",
    "def univariate_analysis(df):\n",
    "    print(\"Univariate Analysis\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"Shape:\\n{df.shape}\\n\")\n",
    "    \n",
    "    # Data Types\n",
    "    print(f\"Data Types:\\n{df.dtypes}\\n\")\n",
    "    \n",
    "    # Missing Values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing Values:\\n{missing_values}\\n\")\n",
    "    \n",
    "    # Duplicates\n",
    "    print(f\"Duplicates:\\n{df.duplicated().sum()}\\n\")\n",
    "    \n",
    "    # Summary statistics for numerical columns\n",
    "    print(\"Summary Statistics for Numerical Columns:\\n\")\n",
    "    print(df.describe(include=[np.number]))\n",
    "    \n",
    "    # Detailed information about NaN values\n",
    "    print(\"\\nDetailed Information about NaN Values:\\n\")\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            print(f\"Column '{col}' has {df[col].isnull().sum()} NaN values, which is {df[col].isnull().mean() * 100:.2f}% of the column.\")\n",
    "\n",
    "# Perform univariate analysis\n",
    "univariate_analysis(original_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis Observations and Insights\n",
    "\n",
    "### Observations:\n",
    "1. **Shape**: The dataset contains 234,500 rows and 20 columns.\n",
    "2. **Data Types**: The dataset includes a mix of numerical and categorical data types.\n",
    "3. **Missing Values**: Several columns have a significant number of missing values:\n",
    "    - 'Academic Pressure', 'CGPA', and 'Study Satisfaction' have 80.10% missing values.\n",
    "    - 'Depression' has 40.00% missing values.\n",
    "    - 'Profession' and 'Job Satisfaction' have around 20% missing values.\n",
    "4. **Duplicates**: There are no duplicate rows in the dataset.\n",
    "5. **Summary Statistics**:\n",
    "    - The 'Age' column ranges from 18 to 60, with a mean of approximately 40.36.\n",
    "    - 'Work/Study Hours' has a wide range from 0 to 12 hours, with a mean of 6.25 hours.\n",
    "    - 'Depression' is a binary column with a mean of 0.18, indicating a low prevalence in the dataset.\n",
    "6. **NaN Values**: Columns with significant NaN values may need imputation or removal depending on their importance.\n",
    "\n",
    "### Insights:\n",
    "- The high percentage of missing values in 'Academic Pressure', 'CGPA', and 'Study Satisfaction' suggests these columns may not be reliable for analysis without imputation.\n",
    "- The dataset appears to be balanced in terms of age distribution, but further analysis is needed to understand the distribution of other categorical variables like 'Gender' and 'City'.\n",
    "- The 'Depression' column, being binary, can be used as a target variable for classification tasks, but missing values need to be addressed.\n",
    "- The dataset's lack of duplicates is beneficial for analysis, ensuring data integrity.\n",
    "\n",
    "### Recommendations:\n",
    "- Consider imputing missing values for critical columns or removing them if they are not essential.\n",
    "- Explore the categorical variables further to understand their distribution and potential impact on the target variable.\n",
    "- Investigate the relationship between 'Work/Study Hours' and 'Depression' to identify any potential correlations.\n",
    "\n",
    "### Next Steps:\n",
    "- Visualize the distribution of key variables to gain further insights.\n",
    "- Perform bivariate analysis to explore relationships between variables.\n",
    "- Address missing values through imputation or removal based on their significance to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def segment_populations(df):\n",
    "    # Students: Have academic-related data\n",
    "    students = df[df['CGPA'].notna()].copy()\n",
    "    \n",
    "    # Workers: Have work-related data but no academic data\n",
    "    workers = df[\n",
    "        (df['Work Pressure'].notna()) & \n",
    "        (df['Job Satisfaction'].notna()) & \n",
    "        (df['CGPA'].isna())\n",
    "    ].copy()\n",
    "    \n",
    "    # Unemployed: No profession but also not students\n",
    "    unemployed = df[\n",
    "        (df['Profession'].isna()) & \n",
    "        (df['CGPA'].isna()) & \n",
    "        (df['Work Pressure'].isna())\n",
    "    ].copy()\n",
    "    \n",
    "    return students, workers, unemployed\n",
    "\n",
    "def prepare_features(df):\n",
    "    # Convert categorical variables\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = ['Gender', 'City', 'Sleep Duration', 'Dietary Habits', \n",
    "                       'Have you ever had suicidal thoughts ?', \n",
    "                       'Family History of Mental Illness', 'Profession']\n",
    "    \n",
    "    # Create numeric DataFrame for correlation analysis\n",
    "    df_numeric = df.copy()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].notna().any():\n",
    "            df_numeric[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Convert remaining numeric columns\n",
    "    numeric_cols = df_numeric.select_dtypes(include=['object']).columns\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to numeric\")\n",
    "    \n",
    "    return df_numeric\n",
    "\n",
    "def analyze_segment(df, segment_name):\n",
    "    print(f\"\\nAnalyzing {segment_name} Population:\")\n",
    "    print(f\"Population size: {len(df)}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nNumerical Features Summary:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Correlation matrix for numeric columns only\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title(f'Correlation Matrix - {segment_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Distribution plots for key metrics\n",
    "    plot_distributions(df, segment_name)\n",
    "\n",
    "def plot_distributions(df, segment_name):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    if n_cols > 0:\n",
    "        fig, axes = plt.subplots(1, n_cols, figsize=(15, 5))\n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:n_cols]):\n",
    "            sns.histplot(data=df, x=col, ax=axes[i])\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "        \n",
    "        plt.suptitle(f'{segment_name} Population - Key Metrics Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main execution\n",
    "students, workers, unemployed = segment_populations(original_data)\n",
    "\n",
    "# Prepare features for each segment\n",
    "students_prepared = prepare_features(students)\n",
    "workers_prepared = prepare_features(workers)\n",
    "unemployed_prepared = prepare_features(unemployed)\n",
    "\n",
    "# Analyze each segment\n",
    "analyze_segment(students_prepared, \"Student\")\n",
    "analyze_segment(workers_prepared, \"Worker\")\n",
    "analyze_segment(unemployed_prepared, \"Unemployed\")\n",
    "\n",
    "# Plot depression rates across segments\n",
    "def plot_depression_rates():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    segments = {\n",
    "        'Students': students,\n",
    "        'Workers': workers,\n",
    "        'Unemployed': unemployed\n",
    "    }\n",
    "    \n",
    "    depression_rates = []\n",
    "    for name, segment in segments.items():\n",
    "        rate = segment['Depression'].mean() * 100\n",
    "        depression_rates.append({'Segment': name, 'Rate': rate})\n",
    "    \n",
    "    depression_df = pd.DataFrame(depression_rates)\n",
    "    sns.barplot(x='Segment', y='Rate', data=depression_df)\n",
    "    plt.title('Depression Rates Across Population Segments')\n",
    "    plt.ylabel('Depression Rate (%)')\n",
    "    plt.show()\n",
    "\n",
    "plot_depression_rates()\n",
    "\n",
    "# Convert pandas DataFrames to Spark DataFrames\n",
    "students_spark = spark.createDataFrame(students)\n",
    "workers_spark = spark.createDataFrame(workers) \n",
    "unemployed_spark = spark.createDataFrame(unemployed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Multivariate Analysis: Population Segments\n",
    "\n",
    "## 1. Student Population (n=46,664)\n",
    "\n",
    "### Key Demographics\n",
    "- **Age**: Mean 25.8 years (SD=4.9), primarily young adults\n",
    "- **Gender**: 55.7% one gender category\n",
    "- **Academic Status**: All students (100%)\n",
    "\n",
    "### Mental Health Profile\n",
    "- **Depression Rate**: 58.6% (highest among segments)\n",
    "- **Suicidal Thoughts**: 63.1% reported having suicidal thoughts\n",
    "- **Family History**: 48.4% reported mental illness history\n",
    "\n",
    "### Academic & Stress Factors\n",
    "- **CGPA**: Mean 7.66 (SD=1.46)\n",
    "- **Academic Pressure**: Mean 3.15/5.0\n",
    "- **Study Satisfaction**: Mean 2.94/5.0\n",
    "- **Financial Stress**: Mean 3.13/5.0 (highest among segments)\n",
    "\n",
    "### Lifestyle Patterns\n",
    "- **Work/Study Hours**: Mean 7.14 hours (highest)\n",
    "- **Sleep Duration**: Mean 13.61 units\n",
    "- **Dietary Habits**: More structured patterns\n",
    "\n",
    "## 2. Worker Population (n=187,794)\n",
    "\n",
    "### Key Demographics\n",
    "- **Age**: Mean 44.0 years (SD=11.0)\n",
    "- **Gender**: 54.7% one gender category\n",
    "- **Professional Status**: Various professions (mean=42.68)\n",
    "\n",
    "### Mental Health Profile\n",
    "- **Depression Rate**: 8.2% (lowest among segments)\n",
    "- **Suicidal Thoughts**: 46.1%\n",
    "- **Family History**: 50.1% reported mental illness history\n",
    "\n",
    "### Work & Stress Factors\n",
    "- **Work Pressure**: Mean 3.00/5.0\n",
    "- **Job Satisfaction**: Mean 2.97/5.0\n",
    "- **Financial Stress**: Mean 2.95/5.0\n",
    "\n",
    "### Lifestyle Patterns\n",
    "- **Work/Study Hours**: Mean 6.03 hours\n",
    "- **Sleep Duration**: Mean 24.40 units\n",
    "- **Dietary Habits**: More varied patterns\n",
    "\n",
    "## 3. Unemployed Population (n=33)\n",
    "\n",
    "### Key Demographics\n",
    "- **Age**: Mean 31.0 years (SD=11.8)\n",
    "- **Gender**: 42.4% one gender category\n",
    "- **Status**: Neither working nor studying\n",
    "\n",
    "### Mental Health Profile\n",
    "- **Depression Rate**: 42.1%\n",
    "- **Suicidal Thoughts**: 63.6%\n",
    "- **Family History**: 24.2% (lowest among segments)\n",
    "\n",
    "### Stress Factors\n",
    "- **Financial Stress**: Mean 3.39/5.0\n",
    "- **Limited Job/Academic Data**: High missing values\n",
    "\n",
    "### Lifestyle Patterns\n",
    "- **Work/Study Hours**: Mean 5.97 hours\n",
    "- **Sleep Duration**: Mean 1.70 units\n",
    "- **Dietary Habits**: Limited variation\n",
    "\n",
    "## Cross-Segment Comparisons\n",
    "\n",
    "### Depression Rates\n",
    "1. Students: 58.6%\n",
    "2. Unemployed: 42.1%\n",
    "3. Workers: 8.2%\n",
    "\n",
    "### Risk Factors by Population\n",
    "- **Students**: Academic pressure, financial stress\n",
    "- **Workers**: Work pressure, family history\n",
    "- **Unemployed**: Financial stress, suicidal thoughts\n",
    "\n",
    "### Protective Factors by Population\n",
    "- **Students**: Structured routine\n",
    "- **Workers**: Job stability, lower depression rates\n",
    "- **Unemployed**: Lower family history of mental illness\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### For Mental Health Professionals\n",
    "1. **Targeted Interventions**:\n",
    "   - Students: Academic stress management\n",
    "   - Workers: Work-life balance programs\n",
    "   - Unemployed: Financial counseling\n",
    "\n",
    "2. **Screening Priorities**:\n",
    "   - High-risk groups (students, unemployed)\n",
    "   - Suicidal thought assessment\n",
    "   - Financial stress evaluation\n",
    "\n",
    "### For Policy Makers\n",
    "1. **Resource Allocation**:\n",
    "   - Student mental health services\n",
    "   - Unemployment support programs\n",
    "   - Workplace mental health initiatives\n",
    "\n",
    "2. **Prevention Strategies**:\n",
    "   - Early intervention programs\n",
    "   - Financial support systems\n",
    "   - Mental health awareness campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for missing values and data distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Apply conditional data suppression based on segment criteria\n",
    "custom_logger(\"ACTION\", \"Filtering data based on segment criteria...\")\n",
    "\n",
    "students_filtered = students[\n",
    "    students['Academic Pressure'].notna() & \n",
    "    students['CGPA'].notna() & \n",
    "    students['Study Satisfaction'].notna()\n",
    "].copy()\n",
    "\n",
    "workers_filtered = workers[\n",
    "    workers['Work Pressure'].notna() & \n",
    "    workers['Job Satisfaction'].notna() & \n",
    "    workers['Profession'].notna()\n",
    "].copy()\n",
    "\n",
    "unemployed_filtered = unemployed[\n",
    "    (unemployed['CGPA'].isna()) &\n",
    "    (unemployed['Work Pressure'].isna()) &\n",
    "    (unemployed['Job Satisfaction'].isna()) &\n",
    "    (unemployed['Profession'].isna())\n",
    "].copy()\n",
    "\n",
    "# Calculate and log data loss percentages\n",
    "students_loss = (1 - len(students_filtered)/len(students)) * 100\n",
    "workers_loss = (1 - len(workers_filtered)/len(workers)) * 100\n",
    "unemployed_loss = (1 - len(unemployed_filtered)/len(unemployed)) * 100\n",
    "\n",
    "custom_logger(\"INFO\", f\"Data loss after filtering:\")\n",
    "custom_logger(\"INFO\", f\"Students: {students_loss:.1f}%\")\n",
    "custom_logger(\"INFO\", f\"Workers: {workers_loss:.1f}%\") \n",
    "custom_logger(\"INFO\", f\"Unemployed: {unemployed_loss:.1f}%\")\n",
    "\n",
    "# Create figure for missing values comparison\n",
    "custom_logger(\"ACTION\", \"Generating visualization of missing values...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Missing values percentages by category after filtering\n",
    "missing_vals = {\n",
    "    'Students': {\n",
    "        'Academic Pressure': (students_filtered['Academic Pressure'].isna().sum() / len(students_filtered)) * 100,\n",
    "        'CGPA': (students_filtered['CGPA'].isna().sum() / len(students_filtered)) * 100,\n",
    "        'Study Satisfaction': (students_filtered['Study Satisfaction'].isna().sum() / len(students_filtered)) * 100,\n",
    "        'Depression': (students_filtered['Depression'].isna().sum() / len(students_filtered)) * 100\n",
    "    },\n",
    "    'Workers': {\n",
    "        'Work Pressure': (workers_filtered['Work Pressure'].isna().sum() / len(workers_filtered)) * 100,\n",
    "        'Job Satisfaction': (workers_filtered['Job Satisfaction'].isna().sum() / len(workers_filtered)) * 100,\n",
    "        'Profession': (workers_filtered['Profession'].isna().sum() / len(workers_filtered)) * 100,\n",
    "        'Depression': (workers_filtered['Depression'].isna().sum() / len(workers_filtered)) * 100\n",
    "    },\n",
    "    'Unemployed': {\n",
    "        'Depression': (unemployed_filtered['Depression'].isna().sum() / len(unemployed_filtered)) * 100,\n",
    "        'Financial Stress': (unemployed_filtered['Financial Stress'].isna().sum() / len(unemployed_filtered)) * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot missing values\n",
    "x_pos = np.arange(len(missing_vals))\n",
    "for i, (category, values) in enumerate(missing_vals.items()):\n",
    "    ax1.bar(i, np.mean(list(values.values())), label=f'{category}\\n(Avg: {np.mean(list(values.values())):.1f}%)')\n",
    "    \n",
    "ax1.set_title('Average Missing Values by Category (After Filtering)', fontsize=14)\n",
    "ax1.set_ylabel('Percentage Missing (%)')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([k for k in missing_vals.keys()])\n",
    "\n",
    "# Calculate and plot depression rates for filtered data\n",
    "custom_logger(\"ACTION\", \"Calculating depression rates by category...\")\n",
    "\n",
    "depression_rates = {\n",
    "    'Students': (students_filtered['Depression'].mean() * 100),\n",
    "    'Workers': (workers_filtered['Depression'].mean() * 100),\n",
    "    'Unemployed': (unemployed_filtered['Depression'].mean() * 100)\n",
    "}\n",
    "\n",
    "ax2.bar(depression_rates.keys(), depression_rates.values(), color='lightcoral')\n",
    "ax2.set_title('Depression Rates by Category (After Filtering)', fontsize=14)\n",
    "ax2.set_ylabel('Depression Rate (%)')\n",
    "\n",
    "for i, v in enumerate(depression_rates.values()):\n",
    "    ax2.text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert filtered DataFrames to Spark DataFrames\n",
    "custom_logger(\"ACTION\", \"Converting filtered data to Spark DataFrames...\")\n",
    "\n",
    "students_spark = spark.createDataFrame(students_filtered)\n",
    "workers_spark = spark.createDataFrame(workers_filtered)\n",
    "unemployed_spark = spark.createDataFrame(unemployed_filtered)\n",
    "\n",
    "# Analyze missing data for each category\n",
    "custom_logger(\"INFO\", \"Detailed Missing Data Analysis by Category:\")\n",
    "\n",
    "categories = {\n",
    "    'Students': students_filtered,\n",
    "    'Workers': workers_filtered, \n",
    "    'Unemployed': unemployed_filtered\n",
    "}\n",
    "\n",
    "# First check for duplicate IDs across categories\n",
    "all_ids = []\n",
    "for category, df in categories.items():\n",
    "    if 'ID' in df.columns:\n",
    "        all_ids.extend(df['ID'].tolist())\n",
    "\n",
    "duplicate_ids = set([id for id in all_ids if all_ids.count(id) > 1])\n",
    "\n",
    "if duplicate_ids:\n",
    "    custom_logger(\"WARNING\", f\"Found {len(duplicate_ids)} duplicate IDs across categories\")\n",
    "    # Remove duplicates keeping first occurrence\n",
    "    total_removed = 0\n",
    "    for category, df in categories.items():\n",
    "        if 'ID' in df.columns:\n",
    "            initial_size = len(df)\n",
    "            categories[category] = df[~df['ID'].isin(duplicate_ids)]\n",
    "            removed_count = initial_size - len(categories[category])\n",
    "            total_removed += removed_count\n",
    "            custom_logger(\"INFO\", f\"Removed {removed_count} duplicate IDs from {category} category\")\n",
    "    custom_logger(\"INFO\", f\"Total of {total_removed} duplicate records removed across all dataframes\")\n",
    "\n",
    "else:\n",
    "    custom_logger(\"SUCCESS\", \"No duplicate IDs found across categories\")\n",
    "\n",
    "# Now proceed with missing value analysis\n",
    "for category, df in categories.items():\n",
    "    custom_logger(\"INFO\", f\"\\n{category} Category:\")\n",
    "    columns_to_drop = []\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            pct_missing = (null_count / len(df)) * 100\n",
    "            custom_logger(\"WARNING\" if pct_missing > 10 else \"INFO\", \n",
    "                         f\"Column '{col}': {null_count} missing values ({pct_missing:.2f}%)\")\n",
    "            if pct_missing > 98:\n",
    "                columns_to_drop.append(col)\n",
    "                custom_logger(\"WARNING\", f\"Column '{col}' will be dropped due to {pct_missing:.2f}% missing values\")\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        categories[category] = df.drop(columns=columns_to_drop)\n",
    "        # Update the original filtered dataframes with dropped columns\n",
    "        if category == 'Students':\n",
    "            students_filtered = categories[category]\n",
    "        elif category == 'Workers':\n",
    "            workers_filtered = categories[category]\n",
    "        elif category == 'Unemployed':\n",
    "            unemployed_filtered = categories[category]\n",
    "        custom_logger(\"INFO\", f\"Dropped {len(columns_to_drop)} columns from {category} category\")\n",
    "\n",
    "custom_logger(\"ACTION\", \"Performing univariate analysis...\")\n",
    "\n",
    "custom_logger(\"INFO\", \"Univariate Analysis Results:\")\n",
    "\n",
    "custom_logger(\"INFO\", \"Students:\")\n",
    "univariate_analysis(students_filtered)\n",
    "\n",
    "custom_logger(\"INFO\", \"Workers:\")\n",
    "univariate_analysis(workers_filtered)\n",
    "\n",
    "custom_logger(\"INFO\", \"Unemployed:\")\n",
    "univariate_analysis(unemployed_filtered)\n",
    "\n",
    "# Create final datasets for each population\n",
    "students_dataset = students_filtered.copy()\n",
    "workers_dataset = workers_filtered.copy() \n",
    "unemployed_dataset = unemployed_filtered.copy()\n",
    "\n",
    "# Save the datasets\n",
    "students_dataset.to_csv('students_final.csv', index=False)\n",
    "workers_dataset.to_csv('workers_final.csv', index=False)\n",
    "unemployed_dataset.to_csv('unemployed_final.csv', index=False)\n",
    "\n",
    "custom_logger(\"SUCCESS\", \"Created and saved final datasets for all three populations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, BatchNormalization, \n",
    "    GlobalMaxPooling1D, Layer, MultiHeadAttention, LayerNormalization, Flatten, Reshape\n",
    ")\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Disable NCCL\n",
    "import os\n",
    "os.environ['NCCL_BLOCKING_WAIT']='0'\n",
    "os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "\n",
    "# ==========================================================================================================================================================#\n",
    "# ======================================================== GPU MALLOC ======================================================================================#\n",
    "# ==========================================================================================================================================================#\n",
    "\n",
    "def freegpu():\n",
    "    try:\n",
    "        result = subprocess.run(['sudo', 'fuser', '-v', '/dev/nvidia*'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        for line in result.stdout.decode('utf-8').split('\\n'):\n",
    "            if '/dev/nvidia' in line:\n",
    "                pid = int(line.split()[-1])\n",
    "                os.kill(pid, 9)\n",
    "        print(\"Freed GPU memory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to free GPU memory: {e}\")\n",
    "\n",
    "# Define a callback to free GPU memory after each epoch\n",
    "class FreeGPUMemoryCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        freegpu()\n",
    "\n",
    "# Clear TensorFlow session\n",
    "def clear_tf_session():\n",
    "    clear_session()\n",
    "    freegpu()\n",
    "\n",
    "# ==========================================================================================================================================================#\n",
    "# =========================================================== RESNET BLOCK =================================================================================#\n",
    "# ==========================================================================================================================================================#\n",
    "\n",
    "# ================================================================[DENSE]===================================================================================#\n",
    "\n",
    "# Define a custom block for ResNet with additional layers for better propagation and backpropagation using Dense layers\n",
    "def resnet_block_dense(x, units, activation='relu'):\n",
    "    \"\"\"\n",
    "    Define a ResNet block using Dense layers with conditional projection and batch normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input tensor.\n",
    "    - units: Number of units in the Dense layers.\n",
    "    - activation: Activation function to use.\n",
    "\n",
    "    Returns:\n",
    "    - A tensor of shape (batch_size, units).\n",
    "\n",
    "    A shortcut is added to the input tensor to improve the propagation and backpropagation of the signal.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    x = Dense(units, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(units, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shortcut = Dense(units, activation=None)(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Define the strategy with HierarchicalCopyAllReduce to avoid NCCL\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    devices=[\"/gpu:0\", \"/gpu:1\"],\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",
    ")\n",
    "\n",
    "print(f\"Number of devices under strategy: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "class CustomAccuracyMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Cast inputs to float32 to ensure type consistency\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
    "        values = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "        self.total.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / self.total\n",
    "\n",
    "class HybridTransformerBlock(kt.HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, hp):\n",
    "        # Simplified hyperparameters\n",
    "        hp_config = {\n",
    "            # Architecture hyperparameters \n",
    "            'cnn_filters': hp.Int('cnn_filters', 16, 64, step=16),\n",
    "            'kernel_size': hp.Int('kernel_size', 3, 5, step=2),\n",
    "            'num_heads': hp.Int('num_heads', 2, 4),\n",
    "            'key_dim': hp.Int('key_dim', 16, 64, step=16),\n",
    "            'fusion_units': hp.Int('fusion_units', 32, 128, step=32),\n",
    "            'fusion_units_2': hp.Int('fusion_units_2', 32, 256, step=32),\n",
    "            'fusion_units_resnet': hp.Int('fusion_units_resnet', 32, 128, step=32),\n",
    "            \n",
    "            # Regularization hyperparameters\n",
    "            'dropout_rate': hp.Float('dropout_rate', 0.1, 0.8, step=0.1),\n",
    "            'dropout_rate_2': hp.Float('dropout_rate_2', 0.1, 0.8, step=0.1),\n",
    "            \n",
    "            # Learning hyperparameters\n",
    "            'batch_size': hp.Choice('batch_size', [32, 64, 128])\n",
    "        }\n",
    "\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=(self.input_shape, 1))\n",
    "        \n",
    "        # Simplified CNN Branch\n",
    "        cnn = layers.Conv1D(\n",
    "            filters=hp_config['cnn_filters'],\n",
    "            kernel_size=hp_config['kernel_size'],\n",
    "            padding='same',\n",
    "            kernel_regularizer=l1_l2(0.001)\n",
    "        )(inputs)\n",
    "        cnn = layers.BatchNormalization()(cnn)\n",
    "        cnn = layers.LeakyReLU()(cnn)\n",
    "        \n",
    "        # Simplified Transformer Branch\n",
    "        transformer = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        attention = MultiHeadAttention(\n",
    "            num_heads=hp_config['num_heads'],\n",
    "            key_dim=hp_config['key_dim']\n",
    "        )(transformer, transformer)\n",
    "        transformer = layers.Add()([transformer, attention])\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion = layers.Concatenate()(\n",
    "            [layers.GlobalAveragePooling1D()(cnn),\n",
    "             layers.GlobalAveragePooling1D()(transformer)]\n",
    "        )\n",
    "        \n",
    "        # Final dense layer\n",
    "        x = layers.Dense(hp_config['fusion_units'], activation='relu',\n",
    "                        kernel_regularizer=l1_l2(0.001))(fusion)\n",
    "        x = layers.Dropout(hp_config['dropout_rate'])(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            x = resnet_block_dense(x, hp_config['fusion_units_resnet'], activation='relu')\n",
    "        \n",
    "        x = layers.Dense(hp_config['fusion_units_2'], activation='relu',\n",
    "                        kernel_regularizer=l1_l2(0.001))(x)\n",
    "        x = layers.Dropout(hp_config['dropout_rate_2'])(x)\n",
    "\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Simple binary crossentropy loss\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data while preserving Depression values for training\"\"\"\n",
    "    try:\n",
    "        custom_logger(\"INFO\", \"DATA PREPROCESSING - PROCESS STARTED\")\n",
    "\n",
    "        # Create copy to avoid modifying original\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Store Depression values and their indices\n",
    "        depression_mask = ~df_copy['Depression'].isna()\n",
    "        train_indices = df_copy[depression_mask].index\n",
    "        predict_indices = df_copy[~depression_mask].index\n",
    "        \n",
    "        train_depression = df_copy.loc[train_indices, 'Depression']\n",
    "\n",
    "        # Identify numerical and categorical columns\n",
    "        numeric_cols = df_copy.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_cols = df_copy.select_dtypes(include=['object']).columns\n",
    "\n",
    "        # Remove id and Depression from feature columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['id', 'Depression']]\n",
    "        categorical_cols = [col for col in categorical_cols if col not in ['id', 'Depression']]\n",
    "\n",
    "        # Handle missing values separately for numeric and categorical columns\n",
    "        for col in numeric_cols:\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].mean())\n",
    "            \n",
    "        for col in categorical_cols:\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].mode()[0])\n",
    "\n",
    "        # One-hot encode categorical columns\n",
    "        df_encoded = pd.get_dummies(df_copy, columns=categorical_cols)\n",
    "\n",
    "        # Scale numeric features\n",
    "        scaler = MinMaxScaler()\n",
    "        df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "        # Standardize scaled features\n",
    "        standard_scaler = StandardScaler()\n",
    "        df_encoded[numeric_cols] = standard_scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "        # Create feature vector column\n",
    "        feature_cols = [col for col in df_encoded.columns if col not in ['id', 'Depression']]\n",
    "        df_encoded['standard_features'] = df_encoded[feature_cols].values.tolist()\n",
    "\n",
    "        # Split into train and predict sets\n",
    "        train_data = df_encoded.loc[train_indices].copy()\n",
    "        train_data['Depression'] = train_depression\n",
    "        \n",
    "        predict_data = df_encoded.loc[predict_indices].copy()\n",
    "\n",
    "        custom_logger(\"INFO\", \"DATA PREPROCESSING - PROCESS COMPLETED\")\n",
    "        return train_data, predict_data\n",
    "\n",
    "    except Exception as e:\n",
    "        custom_logger(\"ERROR\", f\"TASK FAILURE - REASON : {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load datasets\n",
    "students_df = pd.read_csv('students_final.csv')\n",
    "workers_df = pd.read_csv('workers_final.csv') \n",
    "unemployed_df = pd.read_csv('unemployed_final.csv')\n",
    "\n",
    "# Store original Depression values before preprocessing\n",
    "students_original = students_df[~students_df['Depression'].isna()][['id', 'Depression']].copy()\n",
    "workers_original = workers_df[~workers_df['Depression'].isna()][['id', 'Depression']].copy()\n",
    "unemployed_original = unemployed_df[~unemployed_df['Depression'].isna()][['id', 'Depression']].copy()\n",
    "\n",
    "# Preprocess each dataset separately\n",
    "students_train, students_predict = preprocess_data(students_df)\n",
    "workers_train, workers_predict = preprocess_data(workers_df)\n",
    "unemployed_train, unemployed_predict = preprocess_data(unemployed_df)\n",
    "\n",
    "def train_and_predict(train_data, predict_data, segment_name):\n",
    "    # Prepare training data\n",
    "    X = np.array(train_data['standard_features'].tolist())\n",
    "    y = np.array(train_data['Depression'].tolist()).reshape(-1, 1)\n",
    "    \n",
    "    # Cast data to float32\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "    \n",
    "    # Split training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Reshape for Conv1D\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    \n",
    "    # Prepare prediction data\n",
    "    X_predict = np.array(predict_data['standard_features'].tolist())\n",
    "    X_predict = X_predict.astype(np.float32)\n",
    "    X_predict = X_predict.reshape((X_predict.shape[0], X_predict.shape[1], 1))\n",
    "    \n",
    "    # Create datasets with optimized performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    # Training dataset pipeline\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .cache()\\\n",
    "        .shuffle(buffer_size=64)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    # Validation dataset pipeline  \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .cache()\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    # Initialize model builder with input shape\n",
    "    model_builder = HybridTransformerBlock(input_shape=X_train.shape[1])\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='loss', patience=30, restore_best_weights=True),\n",
    "    ]\n",
    "    \n",
    "    # Configure hyperparameter tuning\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective=kt.Objective('val_accuracy', direction='max'),\n",
    "        directory=f'hyperband_tuning_{segment_name}',\n",
    "        project_name=f'depression_prediction_{segment_name}'\n",
    "    )\n",
    "    \n",
    "    # Search for best hyperparameters\n",
    "    tuner.search(train_dataset, epochs=300, validation_data=val_dataset, callbacks=callbacks)\n",
    "    \n",
    "    # Get best hyperparameters and build final model\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = model_builder.build(best_hps)\n",
    "    \n",
    "    # Train final model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=200,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = (model.predict(X_predict) > 0.5).astype(int)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': predict_data['id'],\n",
    "        'Depression': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    return results_df, history\n",
    "\n",
    "# Train models and get predictions for each segment\n",
    "students_predictions, students_history = train_and_predict(students_train, students_predict, 'students')\n",
    "workers_predictions, workers_history = train_and_predict(workers_train, workers_predict, 'workers')\n",
    "unemployed_predictions, unemployed_history = train_and_predict(unemployed_train, unemployed_predict, 'unemployed')\n",
    "\n",
    "# Restore original Depression values\n",
    "def restore_original_values(predictions_df, original_df):\n",
    "    merged = predictions_df.merge(original_df, on='id', how='left', suffixes=('_pred', '_orig'))\n",
    "    merged['Depression'] = merged['Depression_orig'].fillna(merged['Depression_pred'])\n",
    "    return merged[['id', 'Depression']]\n",
    "\n",
    "students_predictions = restore_original_values(students_predictions, students_original)\n",
    "workers_predictions = restore_original_values(workers_predictions, workers_original)\n",
    "unemployed_predictions = restore_original_values(unemployed_predictions, unemployed_original)\n",
    "\n",
    "# Combine all predictions\n",
    "final_predictions = pd.concat([\n",
    "    students_predictions,\n",
    "    workers_predictions, \n",
    "    unemployed_predictions\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save final predictions\n",
    "final_predictions.to_csv('final_predictions.csv', index=False)\n",
    "\n",
    "custom_logger(\"SUCCESS\", \"Generated and saved final predictions for all segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
