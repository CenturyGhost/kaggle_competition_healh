{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"39401c5d-3ad5-481d-bf39-0b168840553b","_uuid":"5fe251cb-3634-4e12-b2b2-bb5fa6f0187f","collapsed":false,"execution":{"iopub.execute_input":"2024-11-14T01:04:02.978122Z","iopub.status.busy":"2024-11-14T01:04:02.977699Z","iopub.status.idle":"2024-11-14T01:04:02.987725Z","shell.execute_reply":"2024-11-14T01:04:02.986779Z","shell.execute_reply.started":"2024-11-14T01:04:02.978081Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/playground-series-s4e11/sample_submission.csv\n","/kaggle/input/playground-series-s4e11/train.csv\n","/kaggle/input/playground-series-s4e11/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Notebook Available here : \n","\n","https://github.com/CenturyGhost/kaggle_competition_healh/blob/main/main.ipynb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-14T01:04:02.989947Z","iopub.status.busy":"2024-11-14T01:04:02.989655Z","iopub.status.idle":"2024-11-14T01:07:29.457741Z","shell.execute_reply":"2024-11-14T01:07:29.456629Z","shell.execute_reply.started":"2024-11-14T01:04:02.989916Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\n","Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.10/site-packages (1.4.7)\n","Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (4.0.0)\n","Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (3.3.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (21.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.32.3)\n","Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.3)\n","Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.30)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n","Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-tuner) (3.1.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (1.4.0)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (13.7.1)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.0.8)\n","Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (3.11.0)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.11.0)\n","Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.3.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->keras-tuner) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->keras-tuner) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n","Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (0.60.0)\n","Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (2.4.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /opt/conda/lib/python3.10/site-packages (from numba) (1.26.4)\n","Collecting pyspark==3.5.0\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=300bb649dce59a31a7976c2fd9595425159d626568ae20411cef7633466625e2\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n","Requirement already satisfied: tensorflow[and-cuda] in /opt/conda/lib/python3.10/site-packages (2.16.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (21.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.32.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (70.0.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.62.2)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.26.4)\n","Collecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n","  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n","  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n","  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n","  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n","  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n","  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n","  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n","  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n","  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n","  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (13.7.1)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.0.8)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.0.4)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow[and-cuda]) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow[and-cuda]) (0.1.2)\n","Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101\n","Requirement already satisfied: tensorflow-probability in /opt/conda/lib/python3.10/site-packages (0.24.0)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (1.4.0)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (1.16.0)\n","Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (1.26.4)\n","Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (5.1.1)\n","Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (3.0.0)\n","Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (0.5.4)\n","Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (0.1.8)\n","Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (4.9.6)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.7)\n","Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\n","Requirement already satisfied: immutabledict in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.26.4)\n","Requirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\n","Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.3)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (16.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\n","Requirement already satisfied: simple-parsing in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.5)\n","Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.14.0)\n","Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.4.0)\n","Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.66.4)\n","Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.0)\n","Requirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.5.1)\n","Requirement already satisfied: etils>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.7.0)\n","Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.12.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.6.1)\n","Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.0)\n","Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.19.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\n","Requirement already satisfied: docstring-parser~=0.15 in /opt/conda/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets) (0.16)\n","Requirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.1)\n","Requirement already satisfied: tensorflow-hub in /opt/conda/lib/python3.10/site-packages (0.16.1)\n","Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub) (1.26.4)\n","Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub) (3.20.3)\n","Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub) (2.16.0)\n","Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.16.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (21.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (70.0.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.62.2)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.3.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.37.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (13.7.1)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.0.4)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n","TensorFlow version: 2.16.1\n","GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"]}],"source":["# Core data science and ML libraries\n","!pip install numpy pandas matplotlib seaborn scikit-learn\n","!pip install keras-tuner optuna\n","!pip install numba termcolor\n","\n","# PySpark installation\n","!pip install pyspark==3.5.0\n","\n","# Additional dependencies that might be needed\n","!pip install tensorflow[and-cuda]\n","!pip install tensorflow-probability\n","!pip install tensorflow-datasets\n","!pip install tensorflow-hub\n","\n","!pip install termcolor\n","\n","!pip install ctypes\n","\n","!pip install tensorflow-mixed-precision\n","\n","!pip install pyspark-dist-keras\n","\n","!pip install scikit-learn\n","\n","# Verify TensorFlow installation and GPU availability\n","import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-14T01:07:29.459512Z","iopub.status.busy":"2024-11-14T01:07:29.459191Z","iopub.status.idle":"2024-11-14T01:07:30.134780Z","shell.execute_reply":"2024-11-14T01:07:30.133299Z","shell.execute_reply.started":"2024-11-14T01:07:29.459469Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'Policy' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set mixed precision policy to improve performance\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Mixed precision uses both 16-bit and 32-bit floating point types\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This can speed up training and reduce memory usage on compatible hardware\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mPolicy\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m set_global_policy(policy)\n","\u001b[0;31mNameError\u001b[0m: name 'Policy' is not defined"]}],"source":["# Set mixed precision policy to improve performance\n","# Mixed precision uses both 16-bit and 32-bit floating point types\n","# This can speed up training and reduce memory usage on compatible hardware\n","policy = Policy('bfloat16')\n","set_global_policy(policy)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-14T01:08:17.544334Z","iopub.status.busy":"2024-11-14T01:08:17.543653Z","iopub.status.idle":"2024-11-14T01:08:17.580895Z","shell.execute_reply":"2024-11-14T01:08:17.579738Z","shell.execute_reply.started":"2024-11-14T01:08:17.544292Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'mixed_precision' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmixed_precision\u001b[49m\u001b[38;5;241m.\u001b[39mset_global_policy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompute dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m policy\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariable dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m policy\u001b[38;5;241m.\u001b[39mvariable_dtype)\n","\u001b[0;31mNameError\u001b[0m: name 'mixed_precision' is not defined"]}],"source":["mixed_precision.set_global_policy('bfloat16')\n","\n","print('Compute dtype: %s' % policy.compute_dtype)\n","print('Variable dtype: %s' % policy.variable_dtype)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-14T01:09:43.145799Z","iopub.status.busy":"2024-11-14T01:09:43.145403Z","iopub.status.idle":"2024-11-14T01:09:43.517175Z","shell.execute_reply":"2024-11-14T01:09:43.515961Z","shell.execute_reply.started":"2024-11-14T01:09:43.145760Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'SparkSession' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Configure Spark with GPU settings for RTX 4090 (24GB) and A5000 (24GB)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR 418 - I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM A TEAPOT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.maxResultSize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.pinnedPool.size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m24G\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.concurrentGpuTasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.gpu.pooling.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.gpu.allocFraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.95\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.explain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.resource.gpu.amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.task.resource.gpu.amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.25\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.incompatibleOps.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.host.spillStorageSize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64G\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.coalescePartitions.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.localShuffleReader.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.gpu.maxAllocFraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.95\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.batchSizeBytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m512M\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.reader.batchSizeRows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.variableRowGroupSize.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     34\u001b[0m gpus \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_logical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpus:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;66;03m# Replicate your computation on multiple GPUs\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"]}],"source":["# Initialize Spark session for the master node\n","# 1 - Use all available local cores for the master node\n","# 2 - Set the application name for the master node\n","# 3 - Configure the driver and executor memory and GPU settings\n","import subprocess\n","import json\n","\n","# Configure Spark with GPU settings for RTX 4090 (24GB) and A5000 (24GB)\n","spark = SparkSession.builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"ERROR 418 - I'M A TEAPOT\") \\\n","    .config(\"spark.driver.memory\", \"50g\") \\\n","    .config(\"spark.executor.memory\", \"50g\") \\\n","    .config(\"spark.driver.maxResultSize\", \"50g\") \\\n","    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n","    .config(\"spark.rapids.memory.pinnedPool.size\", \"24G\") \\\n","    .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\") \\\n","    .config(\"spark.rapids.memory.gpu.pooling.enabled\", \"true\") \\\n","    .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.95\") \\\n","    .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n","    .config(\"spark.executor.resource.gpu.amount\", \"2\") \\\n","    .config(\"spark.task.resource.gpu.amount\", \"0.25\") \\\n","    .config(\"spark.rapids.sql.incompatibleOps.enabled\", \"true\") \\\n","    .config(\"spark.rapids.memory.host.spillStorageSize\", \"64G\") \\\n","    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n","    .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.95\") \\\n","    .config(\"spark.rapids.sql.batchSizeBytes\", \"512M\") \\\n","    .config(\"spark.rapids.sql.reader.batchSizeRows\", \"100000\") \\\n","    .config(\"spark.rapids.sql.variableRowGroupSize.enabled\", \"true\") \\\n","    .getOrCreate()\n","\n","gpus = tf.config.list_logical_devices('GPU')\n","if gpus:\n","  # Replicate your computation on multiple GPUs\n","  c = []\n","  for gpu in gpus:\n","    with tf.device(gpu.name):\n","      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n","      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n","      c.append(tf.matmul(a, b))\n","\n","  with tf.device('/CPU:0'):\n","    matmul_sum = tf.add_n(c)\n","\n","  print(matmul_sum)\n","\n","# Set log level to INFO\n","spark.sparkContext.setLogLevel(\"INFO\")\n","# Configure logging\n","log4jLogger = spark._jvm.org.apache.log4j\n","logger = log4jLogger.LogManager.getLogger(__name__)\n","\n","def custom_logger(level, message):\n","    color = 'white'\n","    if level == \"INFO\":\n","        color = 'cyan'\n","    elif level == \"SUCCESS\":\n","        color = 'green'\n","    elif level == \"ERROR\":\n","        color = 'red'\n","    elif level == \"ACTION\":\n","        color = 'blue'\n","    elif level == \"PROGRESS\" or level == \"WARNING\":\n","        color = 'yellow'\n","    elif level == \"FINAL\":\n","        color = 'magenta'\n","    logger.info(colored(f\"SPARK: {level} - {message}\", color))\n","\n","# Set the environment variable for memory allocator\n","os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n","\n","def initialize_cuda():\n","    cuda = ctypes.CDLL('libcuda.so')\n","    cuInit = cuda.cuInit\n","    cuInit.restype = ctypes.c_int\n","    cuInit.argtypes = [ctypes.c_uint]\n","    cuDeviceGetCount = cuda.cuDeviceGetCount\n","    cuDeviceGetCount.restype = ctypes.c_int\n","    cuDeviceGetCount.argtypes = [ctypes.POINTER(ctypes.c_int)]\n","    cuDeviceGet = cuda.cuDeviceGet\n","    cuDeviceGet.restype = ctypes.c_int\n","    cuDeviceGet.argtypes = [ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n","\n","    res = cuInit(0)\n","    if res != 0:\n","        raise RuntimeError(\"Failed to initialize CUDA\")\n","    device_count = ctypes.c_int()\n","    res = cuDeviceGetCount(ctypes.byref(device_count))\n","    if res != 0:\n","        raise RuntimeError(\"Failed to get device count\")\n","    devices = []\n","    for i in range(device_count.value):\n","        device = ctypes.c_int()\n","        res = cuDeviceGet(ctypes.byref(device), i)\n","        if res != 0:\n","            raise RuntimeError(f\"Failed to get device {i}\")\n","        devices.append(device.value)\n","    return devices\n","\n","def allocate_gpu_memory(size):\n","    cuda = ctypes.CDLL('libcuda.so')\n","    cuda_malloc = cuda.cuMemAlloc\n","    cuda_malloc.restype = ctypes.c_int\n","    cuda_malloc.argtypes = [ctypes.POINTER(ctypes.c_ulonglong), ctypes.c_ulonglong]\n","    ptr = ctypes.c_ulonglong()\n","    res = cuda_malloc(ctypes.byref(ptr), size)\n","    if res != 0:\n","        raise RuntimeError(\"Failed to allocate GPU memory\")\n","    return ptr.value\n","\n","def free_gpu_memory(ptr):\n","    cuda = ctypes.CDLL('libcuda.so')\n","    cuda_free = cuda.cuMemFree\n","    cuda_free.restype = ctypes.c_int\n","    cuda_free.argtypes = [ctypes.c_ulonglong]\n","    res = cuda_free(ptr)\n","    if res != 0:\n","        raise RuntimeError(\"Failed to free GPU memory\")\n","\n","# Check GPU memory usage\n","def check_gpu_memory():\n","    os.system('nvidia-smi')\n","\n","# Initialize CUDA\n","cuda_devices = initialize_cuda()\n","\n","# Check available GPU memory\n","check_gpu_memory()\n","\n","# Ensure TensorFlow uses the GPU by setting memory growth\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}\")\n","\n","# Check and print the number of GPUs available\n","num_gpus = len(tf.config.list_physical_devices('GPU'))\n","print(f\"Num GPUs Available: {num_gpus}\")\n","\n","# Perform a simple TensorFlow operation to verify GPU usage\n","with tf.device('/GPU:0'):\n","    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n","    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=tf.float32)\n","    c = tf.matmul(a, b)\n","print(f\"Result of matrix multiplication on GPU:\\n{c.numpy()}\")\n","\n","def get_gpu_memory_usage():\n","    try:\n","        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","        memory_used = int(result.stdout.decode('utf-8').strip())\n","        return memory_used\n","    except Exception as e:\n","        print(f\"Failed to get GPU memory usage: {e}\")\n","        return None\n","\n","def freegpu():\n","    try:\n","        # Get GPU memory usage before freeing\n","        memory_before = get_gpu_memory_usage()\n","\n","        result = subprocess.run(['sudo', 'fuser', '-v', '/dev/nvidia*'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","        for line in result.stdout.decode('utf-8').split('\\n'):\n","            if '/dev/nvidia' in line:\n","                pid = int(line.split()[-1])\n","                os.kill(pid, 9)\n","\n","        # Get GPU memory usage after freeing\n","        memory_after = get_gpu_memory_usage()\n","\n","        if memory_before is not None and memory_after is not None:\n","            memory_freed = memory_before - memory_after\n","            print(f\"GPU memory has been freed. {memory_freed} MB of GPU memory was released.\")\n","        else:\n","            print(\"GPU memory has been freed.\")\n","    except Exception as e:\n","        print(f\"Failed to free GPU memory: {e}\")\n","\n","# Set the environment variable for XLA flags\n","os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n","\n","# ======================================================================================================\n","# ==============================[SETUP MIRROREDSTRATEGY WITHOUT NCCL]===================================\n","# ======================================================================================================\n","\n","# ------------------------------------------------------------------------------------------------------\n","# Define the strategy with HierarchicalCopyAllReduce to avoid NCCL\n","# ------------------------------------------------------------------------------------------------------\n","strategy = tf.distribute.MirroredStrategy(\n","    devices=[\"/gpu:0\", \"/gpu:1\"],\n","    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",")\n","\n","print(f\"Number of devices under strategy: {strategy.num_replicas_in_sync}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Univariate Analysis\n","\n","This section performs univariate analysis on our dataset to understand the basic characteristics and quality of our data. The analysis includes:\n","\n","- **Shape**: Dimensions of the dataset (rows x columns)\n","- **Data Types**: Type of each column (numeric, categorical, etc.)\n","- **Missing Values**: Count of null/NaN values per column\n","- **Duplicates**: Number of duplicate rows\n","- **Summary Statistics**: Basic statistics for numerical columns including:\n","  - Count\n","  - Mean\n","  - Standard Deviation \n","  - Min/Max values\n","  - Quartile values\n","- **NaN Analysis**: Detailed breakdown of missing values by column with percentages\n","\n","This analysis helps identify potential data quality issues and informs our preprocessing strategy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the processed data for autoencoder training\n","original_data_train = pd.read_csv('/kaggle/input/playground-series-s4e11/test.csv', index_col=False, header=0)\n","original_data_test = pd.read_csv('/kaggle/input/playground-series-s4e11/train.csv', index_col=False, header=0)\n","\n","original_data = pd.concat([original_data_train, original_data_test], ignore_index=True)\n","\n","custom_logger(\"INFO\", \"ORIGINAL DATA - BEFORE PROCESSING\")\n","\n","# Univariate analysis\n","def univariate_analysis(df):\n","    print(\"Univariate Analysis\\n\" + \"=\"*40)\n","    \n","    # Shape\n","    print(f\"Shape:\\n{df.shape}\\n\")\n","    \n","    # Data Types\n","    print(f\"Data Types:\\n{df.dtypes}\\n\")\n","    \n","    # Missing Values\n","    missing_values = df.isnull().sum()\n","    print(f\"Missing Values:\\n{missing_values}\\n\")\n","    \n","    # Duplicates\n","    print(f\"Duplicates:\\n{df.duplicated().sum()}\\n\")\n","    \n","    # Summary statistics for numerical columns\n","    print(\"Summary Statistics for Numerical Columns:\\n\")\n","    print(df.describe(include=[np.number]))\n","    \n","    # Detailed information about NaN values\n","    print(\"\\nDetailed Information about NaN Values:\\n\")\n","    for col in df.columns:\n","        if df[col].isnull().any():\n","            print(f\"Column '{col}' has {df[col].isnull().sum()} NaN values, which is {df[col].isnull().mean() * 100:.2f}% of the column.\")\n","\n","# Perform univariate analysis\n","univariate_analysis(original_data)"]},{"cell_type":"markdown","metadata":{},"source":["# Univariate Analysis Observations and Insights\n","\n","### Observations:\n","1. **Shape**: The dataset contains 234,500 rows and 20 columns.\n","2. **Data Types**: The dataset includes a mix of numerical and categorical data types.\n","3. **Missing Values**: Several columns have a significant number of missing values:\n","    - 'Academic Pressure', 'CGPA', and 'Study Satisfaction' have 80.10% missing values.\n","    - 'Depression' has 40.00% missing values.\n","    - 'Profession' and 'Job Satisfaction' have around 20% missing values.\n","4. **Duplicates**: There are no duplicate rows in the dataset.\n","5. **Summary Statistics**:\n","    - The 'Age' column ranges from 18 to 60, with a mean of approximately 40.36.\n","    - 'Work/Study Hours' has a wide range from 0 to 12 hours, with a mean of 6.25 hours.\n","    - 'Depression' is a binary column with a mean of 0.18, indicating a low prevalence in the dataset.\n","6. **NaN Values**: Columns with significant NaN values may need imputation or removal depending on their importance.\n","\n","### Insights:\n","- The high percentage of missing values in 'Academic Pressure', 'CGPA', and 'Study Satisfaction' suggests these columns may not be reliable for analysis without imputation.\n","- The dataset appears to be balanced in terms of age distribution, but further analysis is needed to understand the distribution of other categorical variables like 'Gender' and 'City'.\n","- The 'Depression' column, being binary, can be used as a target variable for classification tasks, but missing values need to be addressed.\n","- The dataset's lack of duplicates is beneficial for analysis, ensuring data integrity.\n","\n","### Recommendations:\n","- Consider imputing missing values for critical columns or removing them if they are not essential.\n","- Explore the categorical variables further to understand their distribution and potential impact on the target variable.\n","- Investigate the relationship between 'Work/Study Hours' and 'Depression' to identify any potential correlations.\n","\n","### Next Steps:\n","- Visualize the distribution of key variables to gain further insights.\n","- Perform bivariate analysis to explore relationships between variables.\n","- Address missing values through imputation or removal based on their significance to the analysis."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier\n","from pyspark.sql import SparkSession\n","\n","def segment_populations(df):\n","    # Students: Have academic-related data\n","    students = df[df['CGPA'].notna()].copy()\n","    \n","    # Workers: Have work-related data but no academic data\n","    workers = df[\n","        (df['Work Pressure'].notna()) & \n","        (df['Job Satisfaction'].notna()) & \n","        (df['CGPA'].isna())\n","    ].copy()\n","    \n","    # Unemployed: No profession but also not students\n","    unemployed = df[\n","        (df['Profession'].isna()) & \n","        (df['CGPA'].isna()) & \n","        (df['Work Pressure'].isna())\n","    ].copy()\n","    \n","    return students, workers, unemployed\n","\n","def prepare_features(df):\n","    # Convert categorical variables\n","    le = LabelEncoder()\n","    categorical_cols = ['Gender', 'City', 'Sleep Duration', 'Dietary Habits', \n","                       'Have you ever had suicidal thoughts ?', \n","                       'Family History of Mental Illness', 'Profession']\n","    \n","    # Create numeric DataFrame for correlation analysis\n","    df_numeric = df.copy()\n","    \n","    for col in categorical_cols:\n","        if col in df.columns and df[col].notna().any():\n","            df_numeric[col] = le.fit_transform(df[col].astype(str))\n","    \n","    # Convert remaining numeric columns\n","    numeric_cols = df_numeric.select_dtypes(include=['object']).columns\n","    for col in numeric_cols:\n","        try:\n","            df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n","        except:\n","            print(f\"Could not convert {col} to numeric\")\n","    \n","    return df_numeric\n","\n","def analyze_segment(df, segment_name):\n","    print(f\"\\nAnalyzing {segment_name} Population:\")\n","    print(f\"Population size: {len(df)}\")\n","    \n","    # Basic statistics\n","    print(\"\\nNumerical Features Summary:\")\n","    print(df.describe())\n","    \n","    # Correlation matrix for numeric columns only\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    if len(numeric_cols) > 0:\n","        plt.figure(figsize=(12, 8))\n","        sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)\n","        plt.title(f'Correlation Matrix - {segment_name}')\n","        plt.tight_layout()\n","        plt.show()\n","    \n","    # Distribution plots for key metrics\n","    plot_distributions(df, segment_name)\n","\n","def plot_distributions(df, segment_name):\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    n_cols = min(3, len(numeric_cols))\n","    if n_cols > 0:\n","        fig, axes = plt.subplots(1, n_cols, figsize=(15, 5))\n","        if n_cols == 1:\n","            axes = [axes]\n","        \n","        for i, col in enumerate(numeric_cols[:n_cols]):\n","            sns.histplot(data=df, x=col, ax=axes[i])\n","            axes[i].set_title(f'{col} Distribution')\n","        \n","        plt.suptitle(f'{segment_name} Population - Key Metrics Distribution')\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Main execution\n","students, workers, unemployed = segment_populations(original_data)\n","\n","# Prepare features for each segment\n","students_prepared = prepare_features(students)\n","workers_prepared = prepare_features(workers)\n","unemployed_prepared = prepare_features(unemployed)\n","\n","# Analyze each segment\n","analyze_segment(students_prepared, \"Student\")\n","analyze_segment(workers_prepared, \"Worker\")\n","analyze_segment(unemployed_prepared, \"Unemployed\")\n","\n","# Plot depression rates across segments\n","def plot_depression_rates():\n","    plt.figure(figsize=(10, 6))\n","    \n","    segments = {\n","        'Students': students,\n","        'Workers': workers,\n","        'Unemployed': unemployed\n","    }\n","    \n","    depression_rates = []\n","    for name, segment in segments.items():\n","        rate = segment['Depression'].mean() * 100\n","        depression_rates.append({'Segment': name, 'Rate': rate})\n","    \n","    depression_df = pd.DataFrame(depression_rates)\n","    sns.barplot(x='Segment', y='Rate', data=depression_df)\n","    plt.title('Depression Rates Across Population Segments')\n","    plt.ylabel('Depression Rate (%)')\n","    plt.show()\n","\n","plot_depression_rates()\n","\n","# Convert pandas DataFrames to Spark DataFrames\n","students_spark = spark.createDataFrame(students)\n","workers_spark = spark.createDataFrame(workers) \n","unemployed_spark = spark.createDataFrame(unemployed)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create visualizations for missing values and data distributions\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Set up the style\n","plt.style.use('default')\n","sns.set_palette(\"husl\")\n","\n","# Apply conditional data suppression based on segment criteria\n","custom_logger(\"ACTION\", \"Filtering data based on segment criteria...\")\n","\n","students_filtered = students[\n","    students['Academic Pressure'].notna() & \n","    students['CGPA'].notna() & \n","    students['Study Satisfaction'].notna()\n","].copy()\n","\n","workers_filtered = workers[\n","    workers['Work Pressure'].notna() & \n","    workers['Job Satisfaction'].notna() & \n","    workers['Profession'].notna()\n","].copy()\n","\n","unemployed_filtered = unemployed[\n","    (unemployed['CGPA'].isna()) &\n","    (unemployed['Work Pressure'].isna()) &\n","    (unemployed['Job Satisfaction'].isna()) &\n","    (unemployed['Profession'].isna())\n","].copy()\n","\n","# Calculate and log data loss percentages\n","students_loss = (1 - len(students_filtered)/len(students)) * 100\n","workers_loss = (1 - len(workers_filtered)/len(workers)) * 100\n","unemployed_loss = (1 - len(unemployed_filtered)/len(unemployed)) * 100\n","\n","custom_logger(\"INFO\", f\"Data loss after filtering:\")\n","custom_logger(\"INFO\", f\"Students: {students_loss:.1f}%\")\n","custom_logger(\"INFO\", f\"Workers: {workers_loss:.1f}%\") \n","custom_logger(\"INFO\", f\"Unemployed: {unemployed_loss:.1f}%\")\n","\n","# Create figure for missing values comparison\n","custom_logger(\"ACTION\", \"Generating visualization of missing values...\")\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n","\n","# Missing values percentages by category after filtering\n","missing_vals = {\n","    'Students': {\n","        'Academic Pressure': (students_filtered['Academic Pressure'].isna().sum() / len(students_filtered)) * 100,\n","        'CGPA': (students_filtered['CGPA'].isna().sum() / len(students_filtered)) * 100,\n","        'Study Satisfaction': (students_filtered['Study Satisfaction'].isna().sum() / len(students_filtered)) * 100,\n","        'Depression': (students_filtered['Depression'].isna().sum() / len(students_filtered)) * 100\n","    },\n","    'Workers': {\n","        'Work Pressure': (workers_filtered['Work Pressure'].isna().sum() / len(workers_filtered)) * 100,\n","        'Job Satisfaction': (workers_filtered['Job Satisfaction'].isna().sum() / len(workers_filtered)) * 100,\n","        'Profession': (workers_filtered['Profession'].isna().sum() / len(workers_filtered)) * 100,\n","        'Depression': (workers_filtered['Depression'].isna().sum() / len(workers_filtered)) * 100\n","    },\n","    'Unemployed': {\n","        'Depression': (unemployed_filtered['Depression'].isna().sum() / len(unemployed_filtered)) * 100,\n","        'Financial Stress': (unemployed_filtered['Financial Stress'].isna().sum() / len(unemployed_filtered)) * 100\n","    }\n","}\n","\n","# Plot missing values\n","x_pos = np.arange(len(missing_vals))\n","for i, (category, values) in enumerate(missing_vals.items()):\n","    ax1.bar(i, np.mean(list(values.values())), label=f'{category}\\n(Avg: {np.mean(list(values.values())):.1f}%)')\n","    \n","ax1.set_title('Average Missing Values by Category (After Filtering)', fontsize=14)\n","ax1.set_ylabel('Percentage Missing (%)')\n","ax1.set_xticks(x_pos)\n","ax1.set_xticklabels([k for k in missing_vals.keys()])\n","\n","# Calculate and plot depression rates for filtered data\n","custom_logger(\"ACTION\", \"Calculating depression rates by category...\")\n","\n","depression_rates = {\n","    'Students': (students_filtered['Depression'].mean() * 100),\n","    'Workers': (workers_filtered['Depression'].mean() * 100),\n","    'Unemployed': (unemployed_filtered['Depression'].mean() * 100)\n","}\n","\n","ax2.bar(depression_rates.keys(), depression_rates.values(), color='lightcoral')\n","ax2.set_title('Depression Rates by Category (After Filtering)', fontsize=14)\n","ax2.set_ylabel('Depression Rate (%)')\n","\n","for i, v in enumerate(depression_rates.values()):\n","    ax2.text(i, v + 1, f'{v:.1f}%', ha='center')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Convert filtered DataFrames to Spark DataFrames\n","custom_logger(\"ACTION\", \"Converting filtered data to Spark DataFrames...\")\n","\n","students_spark = spark.createDataFrame(students_filtered)\n","workers_spark = spark.createDataFrame(workers_filtered)\n","unemployed_spark = spark.createDataFrame(unemployed_filtered)\n","\n","# Analyze missing data for each category\n","custom_logger(\"INFO\", \"Detailed Missing Data Analysis by Category:\")\n","\n","categories = {\n","    'Students': students_filtered,\n","    'Workers': workers_filtered, \n","    'Unemployed': unemployed_filtered\n","}\n","\n","# First check for duplicate IDs across categories\n","all_ids = []\n","for category, df in categories.items():\n","    if 'ID' in df.columns:\n","        all_ids.extend(df['ID'].tolist())\n","\n","duplicate_ids = set([id for id in all_ids if all_ids.count(id) > 1])\n","\n","if duplicate_ids:\n","    custom_logger(\"WARNING\", f\"Found {len(duplicate_ids)} duplicate IDs across categories\")\n","    # Remove duplicates keeping first occurrence\n","    total_removed = 0\n","    for category, df in categories.items():\n","        if 'ID' in df.columns:\n","            initial_size = len(df)\n","            categories[category] = df[~df['ID'].isin(duplicate_ids)]\n","            removed_count = initial_size - len(categories[category])\n","            total_removed += removed_count\n","            custom_logger(\"INFO\", f\"Removed {removed_count} duplicate IDs from {category} category\")\n","    custom_logger(\"INFO\", f\"Total of {total_removed} duplicate records removed across all dataframes\")\n","\n","else:\n","    custom_logger(\"SUCCESS\", \"No duplicate IDs found across categories\")\n","\n","# Now proceed with missing value analysis\n","for category, df in categories.items():\n","    custom_logger(\"INFO\", f\"\\n{category} Category:\")\n","    columns_to_drop = []\n","    for col in df.columns:\n","        null_count = df[col].isnull().sum()\n","        if null_count > 0:\n","            pct_missing = (null_count / len(df)) * 100\n","            custom_logger(\"WARNING\" if pct_missing > 10 else \"INFO\", \n","                         f\"Column '{col}': {null_count} missing values ({pct_missing:.2f}%)\")\n","            if pct_missing > 98:\n","                columns_to_drop.append(col)\n","                custom_logger(\"WARNING\", f\"Column '{col}' will be dropped due to {pct_missing:.2f}% missing values\")\n","    \n","    if columns_to_drop:\n","        categories[category] = df.drop(columns=columns_to_drop)\n","        # Update the original filtered dataframes with dropped columns\n","        if category == 'Students':\n","            students_filtered = categories[category]\n","        elif category == 'Workers':\n","            workers_filtered = categories[category]\n","        elif category == 'Unemployed':\n","            unemployed_filtered = categories[category]\n","        custom_logger(\"INFO\", f\"Dropped {len(columns_to_drop)} columns from {category} category\")\n","\n","custom_logger(\"ACTION\", \"Performing univariate analysis...\")\n","\n","custom_logger(\"INFO\", \"Univariate Analysis Results:\")\n","\n","custom_logger(\"INFO\", \"Students:\")\n","univariate_analysis(students_filtered)\n","\n","custom_logger(\"INFO\", \"Workers:\")\n","univariate_analysis(workers_filtered)\n","\n","custom_logger(\"INFO\", \"Unemployed:\")\n","univariate_analysis(unemployed_filtered)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
